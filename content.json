{"meta":{"title":"mantch的博客","subtitle":"","description":"","author":"mantch","url":"http://localhost:4000","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-12-24T12:13:00.541Z","updated":"2019-12-24T12:13:00.541Z","comments":true,"path":"404.html","permalink":"http://localhost:4000/404.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"sdf","date":"2019-12-24T09:42:14.000Z","updated":"2019-12-24T09:43:07.620Z","comments":true,"path":"ML/me.html","permalink":"http://localhost:4000/ML/me.html","excerpt":"","text":"s地方sd飞阿斯蒂芬"},{"title":"Linner Regression","date":"2019-12-24T09:51:34.000Z","updated":"2019-12-24T09:51:34.035Z","comments":true,"path":"Linner-Regression/index.html","permalink":"http://localhost:4000/Linner-Regression/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2019-12-24T12:04:36.394Z","updated":"2019-12-24T12:04:36.394Z","comments":true,"path":"tags/index.html","permalink":"http://localhost:4000/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-12-24T12:06:10.333Z","updated":"2019-12-24T12:06:10.333Z","comments":true,"path":"archives/index.html","permalink":"http://localhost:4000/archives/index.html","excerpt":"","text":""},{"title":"404 Not Found","date":"2019-12-25T02:50:58.781Z","updated":"2019-12-25T02:50:58.781Z","comments":true,"path":"friends/index.html","permalink":"http://localhost:4000/friends/index.html","excerpt":"","text":"404 Not Found **很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2019-12-24T12:02:05.757Z","updated":"2019-12-24T12:02:05.757Z","comments":true,"path":"categories/index.html","permalink":"http://localhost:4000/categories/index.html","excerpt":"","text":""},{"title":"关于我的小站","date":"2019-07-28T02:22:12.000Z","updated":"2019-12-24T11:50:20.779Z","comments":true,"path":"about/mantch/index.html","permalink":"http://localhost:4000/about/mantch/index.html","excerpt":"","text":"博主：mantch 公众号：AI-Area 分享自个儿在AI、机器学习、深度学习、NLP、推荐系统等领域的所见、所闻、所思、所想。 公众号的文章都会在我的个人博客里发布，欢迎关注！好货等着你。"},{"title":"项目&资源","date":"2019-07-28T04:30:00.000Z","updated":"2019-12-24T15:33:40.062Z","comments":true,"path":"projects/mantch/index.html","permalink":"http://localhost:4000/projects/mantch/index.html","excerpt":"","text":"一：项目 项目 简介 ML-NLP 此项目是机器学习(Machine Learning)、深度学习(Deep Learning)、NLP面试中常考到的知识点和代码实现，也是作为一个算法工程师必会的理论基础知识。 CodingInterviews2-ByPython 此项目是《剑指offer》第二版里算法面试题的Python3实现版本，作为一本经典书籍，可以时常拿出来看一看、翻一翻、记一记。同时也是为了Python程序员能够更好的通过公司的技术面试，拿到心仪的offer。 二：资源2.1 书籍 《白面机器学习》:https://www.lanzous.com/i56i24f 《剑指offer》:https://www.lanzous.com/i4ya3wd 《数学之美》第二版:https://www.lanzous.com/i3ousch 《推荐系统实战》:https://www.lanzous.com/i6362bi 《自然语言处理综论》第二版:https://www.jianguoyun.com/p/DZIKYLwQj4G5BxiZjboC 2.2 GitHub 《动手学深度学习》TF2.0版:https://github.com/TrickyGo/Dive-into-DL-TensorFlow2.0 数据竞赛top解决方案开源整理：https://github.com/Smilexuhc/Data-Competition-TopSolution 吴恩达老师的课程笔记：https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes"}],"posts":[{"title":"【机器学习】线性回归","slug":"ML/Linner Regression","date":"2019-07-28T04:25:00.000Z","updated":"2019-12-24T10:43:20.274Z","comments":true,"path":"2019/07/28/ML/Linner Regression/","link":"","permalink":"http://localhost:4000/2019/07/28/ML/Linner%20Regression/","excerpt":"线性回归对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。","text":"线性回归对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。 1.什么是线性回归 线性：两个变量之间的关系是一次函数关系的——图象是直线，叫做线性。 非线性：两个变量之间的关系不是一次函数关系的——图象不是直线，叫做非线性。 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算回归到真实值，这就是回归的由来。 2. 能够解决什么样的问题对大量的观测数据进行处理，从而得到比较符合事物内部规律的数学表达式。也就是说寻找到数据与数据之间的规律所在，从而就可以模拟出结果，也就是对结果进行预测。解决的就是通过已知的数据得到未知的结果。例如：对房价的预测、判断信用评价、电影票房预估等。 3. 一般表达式是什么$$Y=wx+b$$ w叫做x的系数，b叫做偏置项。 4. 如何计算4.1 Loss Function–MSE$$J=\\frac{1}{2m}\\sum^{i=1}_{m}(y^{‘}-y)^2$$ 利用梯度下降法找到最小值点，也就是最小误差，最后把 w 和 b 给求出来。 5. 过拟合、欠拟合如何解决使用正则化项，也就是给loss function加上一个参数项，正则化项有L1正则化、L2正则化、ElasticNet。加入这个正则化项好处： 控制参数幅度，不让模型“无法无天”。 限制参数搜索空间 解决欠拟合与过拟合的问题。 5.1 什么是L2正则化(岭回归)方程： $$J=J_0+\\lambda\\sum_{w}w^2$$ $J_0$ 表示上面的 loss function ，在loss function的基础上加入w参数的平方和乘以 $\\lambda$ ，假设： $$L=\\lambda({w_1}^2+{w_2}^2)$$ 回忆以前学过的单位元的方程： $$x^2+y^2=1$$ 正和L2正则化项一样，此时我们的任务变成在L约束下求出J取最小值的解。求解J0的过程可以画出等值线。同时L2正则化的函数L也可以在w1w2的二维平面上画出来。如下图： L表示为图中的黑色圆形，随着梯度下降法的不断逼近，与圆第一次产生交点，而这个交点很难出现在坐标轴上。这就说明了L2正则化不容易得到稀疏矩阵，同时为了求出损失函数的最小值，使得w1和w2无限接近于0，达到防止过拟合的问题。 5.2 什么场景下用L2正则化只要数据线性相关，用LinearRegression拟合的不是很好，需要正则化，可以考虑使用岭回归(L2), 如何输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。 5.3 什么是L1正则化(Lasso回归)L1正则化与L2正则化的区别在于惩罚项的不同： $$J=J_0+\\lambda(|w_1|+|w_2|)$$ 求解J0的过程可以画出等值线。同时L1正则化的函数也可以在w1w2的二维平面上画出来。如下图： 惩罚项表示为图中的黑色棱形，随着梯度下降法的不断逼近，与棱形第一次产生交点，而这个交点很容易出现在坐标轴上。这就说明了L1正则化容易得到稀疏矩阵。 5.4 什么场景下使用L1正则化L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0，从而增强模型的泛化能力 。对于高的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归),或者是要在一堆特征里面找出主要的特征，那么L1正则化(Lasso回归)更是首选了。 5.5 什么是ElasticNet回归ElasticNet综合了L1正则化项和L2正则化项，以下是它的公式： $$min(\\frac{1}{2m}[\\sum_{i=1}^{m}({y_i}^{‘}-y_i)^2+\\lambda\\sum_{j=1}^{n}\\theta_j^2]+\\lambda\\sum_{j=1}^{n}|\\theta|$$ 5.6 ElasticNet回归的使用场景ElasticNet在我们发现用Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候，可以考虑使用ElasticNet回归来综合，得到比较好的结果。 6. 线性回归要求因变量服从正态分布？我们假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布N(0,delta^2)时，因变量则符合正态分布N(ax(i)+b,delta^2)，其中预测函数y=ax(i)+b。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。 在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。 7. 代码实现GitHub：房价预测 8. 什么叫广义线性模型广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。 作者：@mantchs GitHub：https://github.com/NLP-LOVE/ML-NLP","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://localhost:4000/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]}]}