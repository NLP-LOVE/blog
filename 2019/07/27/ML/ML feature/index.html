<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>【机器学习】ML特征工程和优化方法 | mantch的博客</title>
  
  

  

  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.10.1/css/all.min.css">
  

  
  <link rel="shortcut icon" type='image/x-icon' href="https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_15-43-44.png">
  

  
    
<link rel="stylesheet" href="/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  
  
  <div class="cover-wrapper">
    <cover class='cover post half'>
      
        
  <h1 class='title'>mantch</h1>


  <div class="m_search">
    <form name="searchform" class="form u-search-form">
      <input type="text" class="input u-search-input" placeholder="" />
      <i class="icon fas fa-search fa-fw"></i>
    </form>
  </div>

<div class='menu navgation'>
  <ul class='h-list'>
    
      
        <li>
          <a class="nav home" href="/"
            
            
            id="home">
            <i class='fas fa-rss fa-fw'></i>&nbsp;博文
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/categories/"
            
            
            id="categories">
            <i class='fas fa-folder-open fa-fw'></i>&nbsp;分类
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/projects/mantch/"
            
            
            id="projectsmantch">
            <i class='fas fa-code-branch fa-fw'></i>&nbsp;项目&amp;资源
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/about/mantch/"
            
              rel="nofollow"
            
            
            id="aboutmantch">
            <i class='fas fa-info-circle fa-fw'></i>&nbsp;关于
          </a>
        </li>
      
    
  </ul>
</div>

      
    </cover>
    <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>

	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          mantch的博客
        
      </a>
			<div class='menu navgation'>
				<ul class='h-list'>
          
  					
  						<li>
								<a class="nav flat-box" href="/categories/"
                  
                    rel="nofollow"
                  
                  
                  id="categories">
									<i class='fas fa-folder-open fa-fw'></i>&nbsp;分类
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/tags/"
                  
                    rel="nofollow"
                  
                  
                  id="tags">
									<i class='fas fa-hashtag fa-fw'></i>&nbsp;标签
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/archives/"
                  
                    rel="nofollow"
                  
                  
                  id="archives">
									<i class='fas fa-archive fa-fw'></i>&nbsp;归档
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
        
          <li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/"
                
                
                id="home">
								<i class='fas fa-clock fa-fw'></i>&nbsp;近期文章
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/archives/"
                
                  rel="nofollow"
                
                
                id="archives">
								<i class='fas fa-archive fa-fw'></i>&nbsp;文章归档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/projects/mantch/"
                
                
                id="projectsmantch">
								<i class='fas fa-code-branch fa-fw'></i>&nbsp;项目&amp;资源
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/friends/"
                
                  rel="nofollow"
                
                
                id="friends">
								<i class='fas fa-link fa-fw'></i>&nbsp;我的友链
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="https://xaoxuu.com/wiki/material-x/"
                
                  rel="nofollow"
                
                
                id="https:xaoxuu.comwikimaterial-x">
								<i class='fas fa-book fa-fw'></i>&nbsp;主题文档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about/mantch/"
                
                  rel="nofollow"
                
                
                id="aboutmantch">
								<i class='fas fa-info-circle fa-fw'></i>&nbsp;关于小站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>

  </div>


  <div class="l_body">
    <div class='body-wrapper'>
      <div class='l_main'>
  

  <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
    


  <section class='meta'>
    
    
    <div class="meta" id="header-meta">
      
        
  
    <h1 class="title">
      <a href="/2019/07/27/ML/ML%20feature/">
        【机器学习】ML特征工程和优化方法
      </a>
    </h1>
  


      
      <div class='new-meta-box'>
        
          
        
          
            
  <div class='new-meta-item author'>
    
      <a href="http://reverent-montalcini-17e4bf.netlify.com" rel="nofollow">
        
          <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_15-51-11.png">
        
        <p>mantch</p>
      </a>
    
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2019-07-27</p>
  </a>
</div>

          
        
          
            
  
  <div class='new-meta-item category'>
    <a href='/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/' rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>机器学习</p>
    </a>
  </div>


          
        
          
            
  
    <div class="new-meta-item browse busuanzi">
      <a class='notlink'>
        <i class="fas fa-eye" aria-hidden="true"></i>
        <p>
          <span id="busuanzi_value_page_pv">
            <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
          </span>
        </p>
      </a>
    </div>
  


          
        
          
            
  
    <div class="new-meta-item wordcount">
      <a class='notlink'>
        <i class="fas fa-keyboard" aria-hidden="true"></i>
        <p>字数统计:</p>
        <p>11k字</p>
      </a>
    </div>
    <div class="new-meta-item readtime">
      <a class='notlink'>
        <i class="fas fa-hourglass-half" aria-hidden="true"></i>
        <p>阅读时长≈</p>
        <p>39分</p>
      </a>
    </div>
  

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


    <section class="article typo">
      <div class="article-entry" itemprop="articleBody">
        <p>特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。 主要讨论以下两种常用的数据类型。 为了消除…</p>
<a id="more"></a>

<h2 id="1-特征工程有哪些？"><a href="#1-特征工程有哪些？" class="headerlink" title="1. 特征工程有哪些？"></a>1. 特征工程有哪些？</h2><p>特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，<strong>特征工程旨在去除原始数据中的杂质和冗余</strong>，设计更高效的特征以刻画求解的问题与预测模型之间的关系。</p>
<p>主要讨论以下两种常用的数据类型。</p>
<ol>
<li>结构化数据。结构化数据类型可以看作关系型数据库的一张表，每列都 有清晰的定义，包含了数值型、类别型两种基本类型；每一行数据表示一个样本 的信息。</li>
<li>非结构化数据。非结构化数据主要包括文本、图像、音频、视频数据， 其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数 据的大小各不相同。</li>
</ol>
<h3 id="1-1-特征归一化"><a href="#1-1-特征归一化" class="headerlink" title="1.1 特征归一化"></a>1.1 特征归一化</h3><p>为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得 不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果 使用米（m）和千克（kg）作为单位，那么身高特征会在1.6～1.8m的数值范围 内，体重特征会在50～100kg的范围内，分析出来的结果显然会倾向于数值差别比 较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化 （Normalization）处理，使各指标处于同一数值量级，以便进行分析。</p>
<p>对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值 区间内。最常用的方法主要有以下两种。</p>
<ol>
<li><p><strong>线性函数归一化</strong>（Min-Max Scaling）。它对原始数据进行线性变换，使 结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下，其中<em>X</em>为原始数据，$X_{max}、X_{min}$ 分别为数据最大值和最小值。 </p>
<p>$$X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$$</p>
</li>
<li><p><strong>零均值归一化</strong>（Z-Score Normalization）。它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么 归一化公式定义为</p>
<p>$$z=\frac{x-u}{\sigma}$$</p>
</li>
</ol>
<p>优点：<strong>训练数据归一化后，容易更快地通过梯度下降找 到最优解。</strong></p>
<p><img src="http://wx4.sinaimg.cn/mw690/00630Defly1g5cdl44ubjj30gz08i40j.jpg" alt=""></p>
<p>当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模 型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模 型。但对于决策树模型则并不适用。</p>
<h3 id="1-2-类别型特征"><a href="#1-2-类别型特征" class="headerlink" title="1.2 类别型特征"></a>1.2 类别型特征</h3><p>类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、 AB、O）等只在有限选项内取值的特征。类别型特征原始输入通常是字符串形 式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持 向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。</p>
<ol>
<li><p><strong>序号编码</strong></p>
<p>序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为 低、中、高三档，并且存在“高&gt;中&gt;低”的排序关系。序号编码会按照大小关系对 类别型特征赋予一个数值ID，例如高表示为3、中表示为2、低表示为1，转换后依 然保留了大小关系。</p>
</li>
<li><p><strong>独热编码(one-hot)</strong></p>
<p>独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个 取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏 向量，A型血表示为（1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码。</p>
</li>
<li><p>*<em>二进制编码 *</em></p>
<p>二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后 将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下图是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为 010；以此类推可以得到AB型血和O型血的二进制表示。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/00630Defly1g5cdqz4zruj30lf07d74g.jpg" alt=""></p>
</li>
</ol>
<h3 id="1-3-高维组合特征的处理"><a href="#1-3-高维组合特征的处理" class="headerlink" title="1.3 高维组合特征的处理"></a>1.3 高维组合特征的处理</h3><p>为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组 合，构成高阶组合特征。以广告点击预估问题为例，原始数据有语言和类型两种 离散特征，第一张图是语言和类型对点击的影响。为了提高拟合能力，语言和类型可 以组成二阶特征，第二张图是语言和类型的组合特征对点击的影响。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/00630Defly1g5cdvbua1aj30n30kf752.jpg" alt=""></p>
<h3 id="1-4-文本表示模型"><a href="#1-4-文本表示模型" class="headerlink" title="1.4 文本表示模型"></a>1.4 文本表示模型</h3><p>文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领 域的一个重要研究方向。</p>
<ol>
<li><p><strong>词袋模型和N-gram模型</strong></p>
<p>最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子 词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对 应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重。</p>
</li>
<li><p><strong>主题模型</strong></p>
<p>主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布 特性），并且能够计算出每篇文章的主题分布。</p>
</li>
<li><p><strong>词嵌入与深度学习模型</strong></p>
<p>词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维 空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一 维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。</p>
</li>
</ol>
<h3 id="1-5-其它特征工程"><a href="#1-5-其它特征工程" class="headerlink" title="1.5 其它特征工程"></a>1.5 其它特征工程</h3><ol>
<li>如果某个特征当中有<strong>缺失值</strong>，缺失比较少的话，可以使用该特征的平均值或者其它比较靠谱的数据进行填充；缺失比较多的话可以考虑删除该特征。</li>
<li>可以分析特征与结果的相关性，把相关性小的特征去掉。</li>
</ol>
<h3 id="1-6-特征工程脑图"><a href="#1-6-特征工程脑图" class="headerlink" title="1.6 特征工程脑图"></a>1.6 特征工程脑图</h3><p><img src="https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512980743_407.png" alt=""></p>
<h3 id="1-7-标准化和归一化的区别"><a href="#1-7-标准化和归一化的区别" class="headerlink" title="1.7 标准化和归一化的区别"></a>1.7 标准化和归一化的区别</h3><p><strong>归一化</strong>是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。</p>
<p><strong>标准化</strong>是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。</p>
<p>它们的<strong>相同点</strong>在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。</p>
<h2 id="2-机器学习优化方法"><a href="#2-机器学习优化方法" class="headerlink" title="2. 机器学习优化方法"></a>2. 机器学习优化方法</h2><p>优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器 学习算法 = 模型表征 + 模型评估 + 优化算法。其中，优化算法所做的事情就是在 模型表征空间中找到模型评估指标最好的模型。不同的优化算法对应的模型表征 和评估指标不尽相同。</p>
<h3 id="2-1-机器学习常用损失函数"><a href="#2-1-机器学习常用损失函数" class="headerlink" title="2.1 机器学习常用损失函数"></a>2.1 机器学习常用损失函数</h3><p>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。常见的损失函数如下：</p>
<ol>
<li><p><strong>平方损失函数</strong></p>
<p>$$L(Y,f(X))=\sum_{i=1}^{n}(Y-f(X))^2$$</p>
<p>Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和。而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：</p>
<p>$$MSE=\frac{1}{n}\sum_{i=1}^{n}(Y_i^{‘}-Y_i)^2$$</p>
<p>该损失函数一般使用在线性回归当中。</p>
</li>
<li><p><strong>log损失函数</strong></p>
<p><img src="https://wx1.sinaimg.cn/large/00630Defly1g4pvtz3tw9j30et04v0sw.jpg" alt=""></p>
<p>公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0 用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数。</p>
<p>最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。该损失函数一般使用在逻辑回归中。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/00630Defly1g5cf7z1k1rj30b40b4wej.jpg" alt=""></p>
</li>
<li><p><strong>Hinge损失函数</strong></p>
<p>$$L_i=\sum_{j\neq t_i}max(0,f(x_i,W)<em>j-(f(x_i,W)</em>{y_i}-\bigtriangleup))$$</p>
<p>SVM采用的就是Hinge Loss，用于“最大间隔(max-margin)”分类。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/00630Defly1g4w5ezjr64j30se03pmy6.jpg" alt=""></p>
<p>详细见之前<a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/4.%20SVM" target="_blank" rel="noopener">SVM的文章1.2.3</a></p>
</li>
<li><p><strong>0-1损失函数</strong></p>
<p>如果预测值和目标值相等，值为0，如果不相等，值为1。<br>$$<br>L(Y, f(x)) =<br>\begin{cases}<br>1,&amp; Y\ne f(x)\<br>0,&amp; Y = f(x)<br>\end{cases}<br>$$<br>一般的在实际使用中，相等的条件过于严格，可适当放宽条件：</p>
<p>$$<br>L(Y, f(x)) =<br>\begin{cases}<br>1,&amp; |Y-f(x)|\geqslant T\<br>0,&amp; |Y-f(x)|&lt; T<br>\end{cases}<br>$$</p>
</li>
<li><p><strong>绝对值损失函数</strong><br>和0-1损失函数相似，绝对值损失函数表示为：<br>$$<br>L(Y, f(x)) = |Y-f(x)|<br>$$</p>
</li>
<li><p><strong>指数损失函数</strong><br>指数损失函数的标准形式为：<br>$$<br>L(Y, f(x)) = \exp(-Yf(x))<br>$$</p>
<p>例如AdaBoost就是以指数损失函数为损失函数。</p>
</li>
<li><p><strong>均方根误差</strong></p>
<p>$$RMSE=\sqrt{\frac{1}{n}\sum_{i=1}^n(y_i^{‘}-y_i)^2}$$</p>
</li>
<li><p><strong>平均绝对误差</strong></p>
<p>$$MAE=\frac{1}{n}\sum_{i=1}^{n}|y_i^{‘}-y_i|$$</p>
</li>
</ol>
<h3 id="2-2-什么是凸优化"><a href="#2-2-什么是凸优化" class="headerlink" title="2.2 什么是凸优化"></a>2.2 什么是凸优化</h3><p><strong>凸函数</strong>的严格定义为，函数L(·) 是凸函数当且仅当对定义域中的任意两点x，y和任意实数λ∈[0,1]总有：</p>
<p>$$L(\lambda_{}x+(1-\lambda)y)\leq\lambda_{}L(x)+(1-\lambda)L(y)$$</p>
<p>该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任 意一点都不会处于该函数曲面的下方，如下图所示所示。</p>
<p><img src="http://wx4.sinaimg.cn/mw690/00630Defly1g5cfpms6woj30e1049wez.jpg" alt=""></p>
<p>凸优化问题的例子包括支持向量机、线性回归等 线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。</p>
<h3 id="2-3-正则化项"><a href="#2-3-正则化项" class="headerlink" title="2.3 正则化项"></a>2.3 正则化项</h3><p>使用正则化项，也就是给loss function加上一个参数项，正则化项有<strong>L1正则化、L2正则化、ElasticNet</strong>。加入这个正则化项好处：</p>
<ul>
<li>控制参数幅度，不让模型“无法无天”。</li>
<li>限制参数搜索空间</li>
<li>解决欠拟合与过拟合的问题。</li>
</ul>
<p>详细请参考之前的文章：<a href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/Liner%20Regression" target="_blank" rel="noopener">线性回归–第5点</a></p>
<h3 id="2-4-常见的几种最优化方法"><a href="#2-4-常见的几种最优化方法" class="headerlink" title="2.4 常见的几种最优化方法"></a>2.4 常见的几种最优化方法</h3><ol>
<li><p><strong>梯度下降法</strong></p>
<p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：</p>
<p><img src="https://images2017.cnblogs.com/blog/1022856/201709/1022856-20170916201932735-243646199.png" alt=""></p>
<p>缺点：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。</p>
</li>
<li><p><strong>牛顿法</strong></p>
<p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：</p>
<ul>
<li><p>首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f  ‘ (x0)（这里f ‘ 表示函数 f  的导数）。</p>
</li>
<li><p>然后我们计算穿过点(x0,  f  (x0)) 并且斜率为f ‘(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：</p>
<p>$$x<em>f^{‘}(x_0)+f(x_0)-x_0</em>f^{‘}(x_0)=0$$</p>
</li>
<li><p>我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f  (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。</p>
</li>
</ul>
<p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法搜索动态示例图：</p>
<p><img src="https://images2017.cnblogs.com/blog/1022856/201709/1022856-20170916202719078-1588446775.gif" alt=""></p>
<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。<strong>缺点：</strong></p>
<ul>
<li>牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</li>
<li>在高维情况下这个矩阵非常大，计算和存储都是问题。</li>
<li>在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。</li>
<li>目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。</li>
</ul>
</li>
<li><p><strong>拟牛顿法</strong></p>
<p>拟牛顿法是求解非线性优化问题最有效的方法之一，<strong>本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</strong>拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>
</li>
<li><p><strong>共轭梯度法</strong></p>
<p>共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>
<p>具体的实现步骤请参加wiki百科<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB" target="_blank" rel="noopener">共轭梯度法</a>。下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：</p>
<p><img src="http://wx2.sinaimg.cn/mw690/00630Defly1g5ch0r2p48j308z0almy4.jpg" alt=""></p>
</li>
</ol>
<h3 id="2-5-降维方法"><a href="#2-5-降维方法" class="headerlink" title="2.5 降维方法"></a>2.5 降维方法</h3><h4 id="2-5-1-线性判别分析（LDA）"><a href="#2-5-1-线性判别分析（LDA）" class="headerlink" title="2.5.1 线性判别分析（LDA）"></a>2.5.1 线性判别分析（LDA）</h4><p>线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  </p>
<p>LDA分类思想简单总结如下：  </p>
<ol>
<li>多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  </li>
<li>对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  </li>
<li>对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  </li>
</ol>
<p>如果用一句话概括LDA思想，<strong>即“投影后类内方差最小，类间方差最大”。</strong></p>
<p>假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。</p>
<p><img src="http://wx4.sinaimg.cn/mw690/00630Defgy1g5ma79ujplj30qr0ac3z8.jpg" alt=""></p>
<p>左图和右图是两种不同的投影方式。</p>
<p>​    左图思路：让不同类别的平均点距离最远的投影方式。</p>
<p>​    右图思路：让同类别的数据挨得最近的投影方式。</p>
<p>​    从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。</p>
<p>​    以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。</p>
<p><strong>优缺点</strong></p>
<table>
<thead>
<tr>
<th align="center">优缺点</th>
<th align="left">简要说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">优点</td>
<td align="left">1. 可以使用类别的先验知识；<br />2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异；</td>
</tr>
<tr>
<td align="center">缺点</td>
<td align="left">1. LDA不适合对非高斯分布样本进行降维；<br />2. LDA降维最多降到分类数k-1维；<br />3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br />4. LDA可能过度拟合数据。</td>
</tr>
</tbody></table>
<h4 id="2-5-2-主成分分析（PCA）"><a href="#2-5-2-主成分分析（PCA）" class="headerlink" title="2.5.2 主成分分析（PCA）"></a>2.5.2 主成分分析（PCA）</h4><ol>
<li>PCA就是将高维的数据通过线性变换投影到低维空间上去。</li>
<li>投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。</li>
<li>去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。</li>
<li>去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。</li>
<li>对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。</li>
<li>完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。</li>
<li>之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。</li>
</ol>
<p><strong>图解PCA</strong></p>
<p>PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n &lt; m），这n维形成主元，是重构出来最能代表原始数据的正交特征。</p>
<p>​    假设数据集是m个n维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果$n=2$，需要降维到$n’=1$，现在想找到某一维度方向代表这两个维度的数据。下图有$u_1, u_2$两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？</p>
<p><img src="F:/jianguo_syc/GitHub/DeepLearning-500-questions-master/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img/ch2/2.34/1.png" alt=""></p>
<p>从图可看出，$u_1$比$u_2$好，为什么呢？有以下两个主要评价指标：</p>
<ol>
<li>样本点到这个直线的距离足够近。</li>
<li>样本点在这个直线上的投影能尽可能的分开。</li>
</ol>
<p>如果我们需要降维的目标维数是其他任意维，则：</p>
<ol>
<li>样本点到这个超平面的距离足够近。</li>
<li>样本点在这个超平面上的投影能尽可能的分开。</li>
</ol>
<p><strong>优缺点</strong></p>
<table>
<thead>
<tr>
<th align="center">优缺点</th>
<th align="left">简要说明</th>
</tr>
</thead>
<tbody><tr>
<td align="center">优点</td>
<td align="left">1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。</td>
</tr>
<tr>
<td align="center">缺点</td>
<td align="left">1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。</td>
</tr>
</tbody></table>
<h4 id="2-5-3-比较这两种方法"><a href="#2-5-3-比较这两种方法" class="headerlink" title="2.5.3 比较这两种方法"></a>2.5.3 比较这两种方法</h4><p><strong>降维的必要性</strong>：</p>
<ol>
<li>多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。</li>
<li>高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。</li>
<li>过多的变量，对查找规律造成冗余麻烦。</li>
<li>仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。</li>
</ol>
<p><strong>降维的目的</strong>：</p>
<ol>
<li>减少预测变量的个数。</li>
<li>确保这些变量是相互独立的。</li>
<li>提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。</li>
<li>数据在低维下更容易处理、更容易使用。</li>
<li>去除数据噪声。</li>
<li>降低算法运算开销。</li>
</ol>
<p><strong>LDA和PCA区别</strong></p>
<table>
<thead>
<tr>
<th align="center">异同点</th>
<th align="left">LDA</th>
<th align="left">PCA</th>
</tr>
</thead>
<tbody><tr>
<td align="center">相同点</td>
<td align="left">1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布；</td>
<td align="left"></td>
</tr>
<tr>
<td align="center">不同点</td>
<td align="left">有监督的降维方法；</td>
<td align="left">无监督的降维方法；</td>
</tr>
<tr>
<td align="center"></td>
<td align="left">降维最多降到k-1维；</td>
<td align="left">降维多少没有限制；</td>
</tr>
<tr>
<td align="center"></td>
<td align="left">可以用于降维，还可以用于分类；</td>
<td align="left">只用于降维；</td>
</tr>
<tr>
<td align="center"></td>
<td align="left">选择分类性能最好的投影方向；</td>
<td align="left">选择样本点投影具有最大方差的方向；</td>
</tr>
<tr>
<td align="center"></td>
<td align="left">更明确，更能反映样本间差异；</td>
<td align="left">目的较为模糊；</td>
</tr>
</tbody></table>
<h2 id="3-机器学习评估方法"><a href="#3-机器学习评估方法" class="headerlink" title="3. 机器学习评估方法"></a>3. 机器学习评估方法</h2><p>混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。下图为混淆矩阵</p>
<table>
<thead>
<tr>
<th></th>
<th>正类</th>
<th>负类</th>
</tr>
</thead>
<tbody><tr>
<td>预测正确</td>
<td>TP(True Positives)</td>
<td>FP(False Positives)</td>
</tr>
<tr>
<td>预测错误</td>
<td>FN(False Negatives)</td>
<td>TN(True Negatives)</td>
</tr>
</tbody></table>
<h3 id="3-1-准确率-Accuracy"><a href="#3-1-准确率-Accuracy" class="headerlink" title="3.1 准确率(Accuracy)"></a>3.1 准确率(Accuracy)</h3><p><strong>准确率（Accuracy）。</strong>顾名思义，就是所有的预测正确（正类负类）的占总的比重。</p>
<p>$$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$</p>
<p>准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比 如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确 率。所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准 确率的最主要因素。</p>
<h3 id="3-2-精确率（Precision）"><a href="#3-2-精确率（Precision）" class="headerlink" title="3.2 精确率（Precision）"></a>3.2 精确率（Precision）</h3><p><strong>精确率（Precision）</strong>，查准率。即正确预测为正的占全部预测为正的比例。个人理解：真正正确的占所有预测为正的比例。</p>
<p>$$Precision=\frac{TP}{TP+FP}$$</p>
<h3 id="3-3-召回率-Recall"><a href="#3-3-召回率-Recall" class="headerlink" title="3.3 召回率(Recall)"></a>3.3 召回率(Recall)</h3><p><strong>召回率（Recall）</strong>，查全率。即正确预测为正的占全部实际为正的比例。个人理解：真正正确的占所有实际为正的比例。</p>
<p>$$Recall=\frac{TP}{TP+FN}$$</p>
<p>为了综合评估一个排序模型的好坏，不仅要看模型在不同 Top N下的Precision@N和Recall@N，而且最好绘制出模型的P-R（Precision- Recall）曲线。这里简单介绍一下P-R曲线的绘制方法。</p>
<p>P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲 线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本， 小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R 曲线是通过将阈值从高到低移动而生成的。下图是P-R曲线样例图，其中实线代表 模型A的P-R曲线，虚线代表模型B的P-R曲线。原点附近代表当阈值最大时模型的 精确率和召回率。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/00630Defly1g5e4ocvl8aj30ep0d275m.jpg" alt=""></p>
<p>由图可见，当召回率接近于0时，模型A的精确率为0.9，模型B的精确率是1， 这说明模型B得分前几位的样本全部是真正的正样本，而模型A即使得分最高的几 个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋 势。但是，当召回率为1时，模型A的精确率反而超过了模型B。<strong>这充分说明，只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过P-R曲线的 整体表现，才能够对模型进行更为全面的评估。</strong></p>
<h3 id="3-4-F1值-H-mean值"><a href="#3-4-F1值-H-mean值" class="headerlink" title="3.4 F1值(H-mean值)"></a>3.4 F1值(H-mean值)</h3><p>F1值（H-mean值）。F1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。</p>
<p>$$\frac{2}{F_1}=\frac{1}{Precision}+\frac{1}{Recall}$$</p>
<p>$$F_1=\frac{2PR}{P+R}=\frac{2TP}{2TP+FP+FN}$$</p>
<h3 id="3-4-ROC曲线"><a href="#3-4-ROC曲线" class="headerlink" title="3.4 ROC曲线"></a>3.4 ROC曲线</h3><h4 id="3-4-1-ROC曲线"><a href="#3-4-1-ROC曲线" class="headerlink" title="3.4.1 ROC曲线"></a>3.4.1 ROC曲线</h4><p>ROC曲线。接收者操作特征曲线（receiver operating characteristic curve），是反映敏感性和特异性连续变量的综合指标，ROC曲线上每个点反映着对同一信号刺激的感受性。下图是ROC曲线例子。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/00630Defly1g5e4fnjx1dj308w08waby.jpg" alt=""></p>
<p>横坐标：1-Specificity，伪正类率(False positive rate，FPR，FPR=FP/(FP+TN))，预测为正但实际为负的样本占所有负例样本的比例；</p>
<p>纵坐标：Sensitivity，真正类率(True positive rate，TPR，TPR=TP/(TP+FN))，预测为正且实际为正的样本占所有正例样本的比例。</p>
<p><strong>真正的理想情况</strong>，TPR应接近1，FPR接近0，即图中的（0,1）点。<strong>ROC曲线越靠拢（0,1）点，越偏离45度对角线越好</strong>。</p>
<h4 id="3-4-2-AUC值"><a href="#3-4-2-AUC值" class="headerlink" title="3.4.2 AUC值"></a>3.4.2 AUC值</h4><p>AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。</p>
<p>从AUC判断分类器（预测模型）优劣的标准：</p>
<ul>
<li>AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。</li>
<li>0.5 &lt; AUC &lt; 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</li>
<li>AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。</li>
<li>AUC &lt; 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。</li>
</ul>
<p><strong>一句话来说，AUC值越大的分类器，正确率越高。</strong></p>
<p><strong>如何计算AUC</strong></p>
<ul>
<li>将坐标点按照横坐标FPR排序 。</li>
<li>计算第$i$个坐标点和第$i+1$个坐标点的间距$dx$ 。 </li>
<li>获取第$i$或者$i+1$个坐标点的纵坐标y。</li>
<li>计算面积微元$ds=ydx$。</li>
<li>对面积微元进行累加，得到AUC。</li>
</ul>
<h4 id="3-4-3-为什么使用Roc和Auc评价分类器"><a href="#3-4-3-为什么使用Roc和Auc评价分类器" class="headerlink" title="3.4.3 为什么使用Roc和Auc评价分类器"></a>3.4.3 为什么使用Roc和Auc评价分类器</h4><p>因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。</p>
<h3 id="3-5-余弦距离和欧式距离"><a href="#3-5-余弦距离和欧式距离" class="headerlink" title="3.5 余弦距离和欧式距离"></a>3.5 余弦距离和欧式距离</h3><p><strong>余弦距离：</strong>$cos(A,B)=\frac{A*B}{||A||_2||B||_2}$</p>
<p><strong>欧式距离：</strong>在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。</p>
<p>对于两个向量A和B，余弦距离关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值 范围是[−1,1]。当一对文本相似度的长度差距很大、但内容相近时，如果使用词频 或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦 相似度的话，它们之间的夹角可能很小，因而相似度高。此外，在文本、图像、 视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保 持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的 影响，范围不固定，并且含义也比较模糊。</p>
<h3 id="3-6-A-B测试"><a href="#3-6-A-B测试" class="headerlink" title="3.6 A/B测试"></a>3.6 A/B测试</h3><p>AB测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。</p>
<h3 id="3-7-模型评估方法"><a href="#3-7-模型评估方法" class="headerlink" title="3.7 模型评估方法"></a>3.7 模型评估方法</h3><ol>
<li><p><strong>Holdout检验</strong></p>
<p>Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分 成训练集和验证集两部分。比方说，对于一个点击率预测模型，我们把样本按照 70%～30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型 验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。</p>
<p>Holdout 检验的缺点很明显，即在验证集上计算出来的最后评估指标与原始分 组有很大关系。为了消除随机性，研究者们引入了“交叉检验”的思想。</p>
</li>
<li><p><strong>交叉检验</strong></p>
<p>k-fold交叉验证：首先将全部样本划分成k个大小相等的样本子集；依次遍历 这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的 训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验 中，k经常取10。</p>
</li>
<li><p><strong>自助法</strong></p>
<p>不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行 模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步 减小，这可能会影响模型训练效果。有没有能维持训练集样本规模的验证方法 呢？自助法可以比较好地解决这个问题。 </p>
<p>自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行n次有 放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采 样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验 证，这就是自助法的验证过程。</p>
</li>
</ol>
<h3 id="3-8-超参数调优"><a href="#3-8-超参数调优" class="headerlink" title="3.8 超参数调优"></a>3.8 超参数调优</h3><p>为了进行超参数调优，我们一般会采用网格搜索、随机搜索、贝叶斯优化等 算法。在具体介绍算法之前，需要明确超参数搜索算法一般包括哪几个要素。一 是目标函数，即算法需要最大化/最小化的目标；二是搜索范围，一般通过上限和 下限来确定；三是算法的其他参数，如搜索步长。</p>
<ul>
<li><strong>网格搜索</strong>，可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范 围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格 搜索有很大概率找到全局最优值。然而，<strong>这种搜索方案十分消耗计算资源和时间</strong>，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然 后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低 所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最 优值。</li>
<li><strong>随机搜索</strong>，随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有 值，而是在搜索范围中随机选取样本点。它的理论依据是，如果样本点集足够 大，那么通过随机采样也能大概率地找到全局最优值，或其近似值。随机搜索一 般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。</li>
<li><strong>贝叶斯优化算法</strong>，贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全 不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形 状进行学习，找到使目标函数向全局最优值提升的参数。</li>
</ul>
<h3 id="3-9-过拟合和欠拟合"><a href="#3-9-过拟合和欠拟合" class="headerlink" title="3.9 过拟合和欠拟合"></a>3.9 过拟合和欠拟合</h3><p>过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是 模型在训练集上的表现很好，但在测试集和新数据上的表现较差。欠拟合指的是 模型在训练和预测时表现都不好的情况。下图形象地描述了过拟合和欠拟合的区别。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/00630Defly1g5e5937nuej30hw05i3zl.jpg" alt=""></p>
<ol>
<li><strong>防止过拟合：</strong><ul>
<li>从数据入手，获得更多的训练数据。</li>
<li>降低模型复杂度。</li>
<li>正则化方法，给模型的参数加上一定的正则约束。</li>
<li>集成学习方法，集成学习是把多个模型集成在一起。</li>
</ul>
</li>
<li><strong>防止欠拟合：</strong><ul>
<li>添加新特征。</li>
<li>增加模型复杂度。</li>
<li>减小正则化系数。</li>
</ul>
</li>
</ol>
<h2 id="4-检验方法"><a href="#4-检验方法" class="headerlink" title="4. 检验方法"></a>4. 检验方法</h2><h3 id="4-1-KS检验"><a href="#4-1-KS检验" class="headerlink" title="4.1 KS检验"></a>4.1 KS检验</h3><p>Kolmogorov-Smirnov检验是基于累计分布函数的，用于检验一个分布是否符合某种理论分布或比较两个经验分布是否有显著差异。</p>
<ul>
<li>单样本K-S检验是用来检验一个数据的观测经验分布是否符合已知的理论分布。</li>
<li>两样本K-S检验由于对两样本的经验分布函数的位置和形状参数的差异都敏感，所以成为比较两样本的最有用且最常用的非参数方法之一。</li>
</ul>
<p>检验统计量为：$D_r=max_x|F_n(x)-F(x)|$</p>
<p>其中  $F_n(x)$为观察序列值，$F(x)$为理论序列值或另一观察序列值。</p>
<h3 id="4-2-T检验"><a href="#4-2-T检验" class="headerlink" title="4.2 T检验"></a>4.2 T检验</h3><p>T检验，也称student t检验，主要用户样本含量较小，总体标准差未知的正态分布。</p>
<p>t检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。</p>
<p>t检验分为单总体检验和双总体检验。</p>
<h3 id="4-3-F检验"><a href="#4-3-F检验" class="headerlink" title="4.3 F检验"></a>4.3 F检验</h3><p>T检验和F检验的由来：为了确定从样本中的统计结果推论到总体时所犯错的概率。F检验又叫做联合假设检验，也称方差比率检验、方差齐性检验。是由英国统计学家Fisher提出。通过比较两组数据的方差，以确定他们的精密度是否有显著性差异。</p>
<h3 id="4-4-Grubbs检验"><a href="#4-4-Grubbs检验" class="headerlink" title="4.4 Grubbs检验"></a>4.4 Grubbs检验</h3><p>一组测量数据中，如果个别数据偏离平均值很远，那么称这个数据为“可疑值”。用格拉布斯法判断，能将“可疑值”从测量数据中剔除。</p>
<h3 id="4-5-卡方检验"><a href="#4-5-卡方检验" class="headerlink" title="4.5 卡方检验"></a>4.5 卡方检验</h3><p>卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。</p>
<ol>
<li><p>提出原假设H0：总体X的分布函数F(x)；</p>
</li>
<li><p>将总体x的取值范围分成k个互不相交的小区间A1-Ak；</p>
</li>
<li><p>把落入第i个区间Ai的样本的个数记做fi，成为组频数，f1+f2+f3+…+fk = n；</p>
</li>
<li><p>当H0为真时，根据假设的总体理论分布，可算出总体X的值落入第i个小区间Ai的概率pi，于是n*pi就是落入第i个小区间Ai的样本值的理论频数；</p>
</li>
<li><p>当H0为真时，n次试验中样本落入第i个小区间Ai的频率fi/n与概率pi应该很接近。基于这种思想，皮尔逊引入检测统计量：</p>
<p>$$x^2=\sum_{i=1}^{k}\frac{(f_i-np_i)^2}{np_i}$$</p>
<p>在H0假设成立的情况下服从自由度为k-1的卡方分布。</p>
</li>
</ol>
<p><strong>KS检验与卡方检验</strong></p>
<p><strong>相同点：</strong>都采用实际频数和期望频数只差进行检验</p>
<p><strong>不同点：</strong></p>
<ul>
<li>卡方检验主要用于类别数据，而KS检验主要用于有计量单位的连续和定量数据。</li>
<li>卡方检验也可以用于定量数据，但必须先将数据分组才能获得实际的观测频数，而KS检验能直接对原始数据进行检验，所以它对数据的利用比较完整。</li>
</ul>
<h2 id="5-常用分类算法的优缺点？"><a href="#5-常用分类算法的优缺点？" class="headerlink" title="5. 常用分类算法的优缺点？"></a>5. 常用分类算法的优缺点？</h2><table>
<thead>
<tr>
<th align="left">算法</th>
<th align="left">优点</th>
<th align="left">缺点</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Bayes 贝叶斯分类法</td>
<td align="left">1）所需估计的参数少，对于缺失数据不敏感。<br />2）有着坚实的数学基础，以及稳定的分类效率。</td>
<td align="left">1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br />2）需要知道先验概率。<br />3）分类决策存在错误率。</td>
</tr>
<tr>
<td align="left">Decision Tree决策树</td>
<td align="left">1）不需要任何领域知识或参数假设。<br />2）适合高维数据。<br />3）简单易于理解。<br />4）短时间内处理大量数据，得到可行且效果较好的结果。<br />5）能够同时处理数据型和常规性属性。</td>
<td align="left">1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br />2）易于过拟合。<br />3）忽略属性之间的相关性。<br />4）不支持在线学习。</td>
</tr>
<tr>
<td align="left">SVM支持向量机</td>
<td align="left">1）可以解决小样本下机器学习的问题。<br />2）提高泛化性能。<br />3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br />4）避免神经网络结构选择和局部极小的问题。</td>
<td align="left">1）对缺失数据敏感。<br />2）内存消耗大，难以解释。<br />3）运行和调参略烦人。</td>
</tr>
<tr>
<td align="left">KNN K近邻</td>
<td align="left">1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br />2）可用于非线性分类；<br /> 3）训练时间复杂度为O(n)； <br />4）准确度高，对数据没有假设，对outlier不敏感；</td>
<td align="left">1）计算量太大。<br />2）对于样本分类不均衡的问题，会产生误判。<br />3）需要大量的内存。<br />4）输出的可解释性不强。</td>
</tr>
<tr>
<td align="left">Logistic Regression逻辑回归</td>
<td align="left">1）速度快。<br />2）简单易于理解，直接看到各个特征的权重。<br />3）能容易地更新模型吸收新的数据。<br />4）如果想要一个概率框架，动态调整分类阀值。</td>
<td align="left">特征处理复杂。需要归一化和较多的特征工程。</td>
</tr>
<tr>
<td align="left">Neural Network 神经网络</td>
<td align="left">1）分类准确率高。<br />2）并行处理能力强。<br />3）分布式存储和学习能力强。<br />4）鲁棒性较强，不易受噪声影响。</td>
<td align="left">1）需要大量参数（网络拓扑、阀值、阈值）。<br />2）结果难以解释。<br />3）训练时间过长。</td>
</tr>
<tr>
<td align="left">Adaboosting</td>
<td align="left">1）adaboost是一种有很高精度的分类器。<br />2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br />3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br />4）简单，不用做特征筛选。<br />5）不用担心overfitting。</td>
<td align="left">对outlier比较敏感</td>
</tr>
</tbody></table>
<h2 id="6-参考文献"><a href="#6-参考文献" class="headerlink" title="6. 参考文献"></a>6. 参考文献</h2><p><a href="https://www.lanzous.com/i56i24f" target="_blank" rel="noopener">百面机器学习</a></p>
<h2 id="7-机器学习系列教程"><a href="#7-机器学习系列教程" class="headerlink" title="7. 机器学习系列教程"></a>7. 机器学习系列教程</h2><p>GitHub：<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>
<blockquote>
<p>作者：<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">@mantchs</a></p>
<p>GitHub：<a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>
<p>欢迎大家加入讨论！共同完善此项目！群号：【541954936】<a target="_blank" href="//shang.qq.com/wpa/qunwpa?idkey=863f915b9178560bd32ca07cd090a7d9e6f5f90fcff5667489697b1621cecdb3"><img border="0" src="http://pub.idqqimg.com/wpa/images/group.png" alt="NLP面试学习群" title="NLP面试学习群"></a></p>
</blockquote>

      </div>
      
      
        <br>
        


  <section class='meta' id="footer-meta">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2019-12-25T13:43:22+08:00">
  <a class='notlink'>
    <i class="fas fa-clock" aria-hidden="true"></i>
    <p>更新于 2019年12月25日</p>
  </a>
</div>

        
      
        
          

        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="QQ好友" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=http://reverent-montalcini-17e4bf.netlify.com/2019/07/27/ML/ML%20feature/&title=【机器学习】ML特征工程和优化方法 | mantch的博客&pics=https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_15-51-11.png&summary=特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。 主要讨论以下两种常用的数据类型。 为了消除…"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="QQ空间" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://reverent-montalcini-17e4bf.netlify.com/2019/07/27/ML/ML%20feature/&title=【机器学习】ML特征工程和优化方法 | mantch的博客&pics=https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_15-51-11.png&summary=特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。 主要讨论以下两种常用的数据类型。 为了消除…"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="微博" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=http://reverent-montalcini-17e4bf.netlify.com/2019/07/27/ML/ML%20feature/&title=【机器学习】ML特征工程和优化方法 | mantch的博客&pics=https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_15-51-11.png&summary=特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。 主要讨论以下两种常用的数据类型。 为了消除…"
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


      
      
          <div class="prev-next">
              
                  <section class="prev">
                      <span class="art-item-left">
                          <h6><i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页</h6>
                          <h4>
                              <a href="/2019/07/28/ML/Topic%20Model/" rel="prev" title="【机器学习】主题模型">
                                
                                    【机器学习】主题模型
                                
                              </a>
                          </h4>
                          
                      </span>
                  </section>
              
              
                  <section class="next">
                      <span class="art-item-right" aria-hidden="true">
                          <h6>下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i></h6>
                          <h4>
                              <a href="/2019/07/25/ML/Clustering/" rel="prev" title="【机器学习】聚类算法">
                                  
                                      【机器学习】聚类算法
                                  
                              </a>
                          </h4>
                          
                      </span>
                  </section>
              
          </div>
      
    </section>
  </article>



  <!-- 显示推荐文章和评论 -->



  <article class="post white-box comments">
    <section class="article typo">
      <h4><i class="fas fa-comments fa-fw" aria-hidden="true"></i>&nbsp;评论</h4>
      
      
      
        <section id="comments">
          <div id="gitalk-container"></div>
        </section>
      
      
    </section>
  </article>






<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script>
    window.subData = {
      title: '【机器学习】ML特征工程和优化方法',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
    
    
      
      
        
          
          
            
              <section class='widget author'>
  <div class='content pure'>
    
      <div class='avatar'>
        <img class='avatar' src='https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_16-7-39.png'/>
      </div>
    
    
      <div class='text'>
        
        
        
          <p><span id="jinrishici-sentence">mantch的博客</span></p>
          <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
        
      </div>
    
    
      <div class="social-wrapper">
        
          
            <a href="mailto:mantchs@163.com"
              class="social fas fa-envelope flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/NLP-LOVE"
              class="social fab fa-github flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
  </div>
</section>

            
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
            
              <section class='widget plain'>
  
<header class='pure'>
  <div><i class="fas fa-file fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;AI-Area</div>
  
</header>

  <div class='content pure'>
    <p>微信公众号(最新文章分享)，请关注!<br><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-12-24_16-17-47.png" alt=""></p>

  </div>
</section>

            
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
            
              
  <section class='widget toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;本文目录</div>
  
    <!-- <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div> -->
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-特征工程有哪些？"><span class="toc-text">1. 特征工程有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-特征归一化"><span class="toc-text">1.1 特征归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-类别型特征"><span class="toc-text">1.2 类别型特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-高维组合特征的处理"><span class="toc-text">1.3 高维组合特征的处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-文本表示模型"><span class="toc-text">1.4 文本表示模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-其它特征工程"><span class="toc-text">1.5 其它特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-特征工程脑图"><span class="toc-text">1.6 特征工程脑图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-7-标准化和归一化的区别"><span class="toc-text">1.7 标准化和归一化的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-机器学习优化方法"><span class="toc-text">2. 机器学习优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-机器学习常用损失函数"><span class="toc-text">2.1 机器学习常用损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-什么是凸优化"><span class="toc-text">2.2 什么是凸优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-正则化项"><span class="toc-text">2.3 正则化项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-常见的几种最优化方法"><span class="toc-text">2.4 常见的几种最优化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-降维方法"><span class="toc-text">2.5 降维方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-1-线性判别分析（LDA）"><span class="toc-text">2.5.1 线性判别分析（LDA）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-2-主成分分析（PCA）"><span class="toc-text">2.5.2 主成分分析（PCA）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-3-比较这两种方法"><span class="toc-text">2.5.3 比较这两种方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-机器学习评估方法"><span class="toc-text">3. 机器学习评估方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-准确率-Accuracy"><span class="toc-text">3.1 准确率(Accuracy)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-精确率（Precision）"><span class="toc-text">3.2 精确率（Precision）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-召回率-Recall"><span class="toc-text">3.3 召回率(Recall)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-F1值-H-mean值"><span class="toc-text">3.4 F1值(H-mean值)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-ROC曲线"><span class="toc-text">3.4 ROC曲线</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-ROC曲线"><span class="toc-text">3.4.1 ROC曲线</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-AUC值"><span class="toc-text">3.4.2 AUC值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-为什么使用Roc和Auc评价分类器"><span class="toc-text">3.4.3 为什么使用Roc和Auc评价分类器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-余弦距离和欧式距离"><span class="toc-text">3.5 余弦距离和欧式距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-A-B测试"><span class="toc-text">3.6 A&#x2F;B测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-模型评估方法"><span class="toc-text">3.7 模型评估方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-8-超参数调优"><span class="toc-text">3.8 超参数调优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-9-过拟合和欠拟合"><span class="toc-text">3.9 过拟合和欠拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-检验方法"><span class="toc-text">4. 检验方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-KS检验"><span class="toc-text">4.1 KS检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-T检验"><span class="toc-text">4.2 T检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-F检验"><span class="toc-text">4.3 F检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Grubbs检验"><span class="toc-text">4.4 Grubbs检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-卡方检验"><span class="toc-text">4.5 卡方检验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-常用分类算法的优缺点？"><span class="toc-text">5. 常用分类算法的优缺点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-参考文献"><span class="toc-text">6. 参考文献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-机器学习系列教程"><span class="toc-text">7. 机器学习系列教程</span></a></li></ol>
    </div>
  </section>


            
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
            
              <section class='widget grid'>
  
<header class='pure'>
  <div><i class="fas fa-map-signs fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;站内导航</div>
  
</header>

  <div class='content pure'>
    <ul class="grid navgation">
      
        <li><a class="flat-box" title="/" href="/"
          
          
          id="home">
          
            <i class="fas fa-clock fa-fw" aria-hidden="true"></i>
          
          近期文章
        </a></li>
      
        <li><a class="flat-box" title="/archives/" href="/archives/"
          
            rel="nofollow"
          
          
          id="archives">
          
            <i class="fas fa-archive fa-fw" aria-hidden="true"></i>
          
          文章归档
        </a></li>
      
        <li><a class="flat-box" title="/projects/mantch/" href="/projects/mantch/"
          
          
          id="projectsmantch">
          
            <i class="fas fa-code-branch fa-fw" aria-hidden="true"></i>
          
          项目&资源
        </a></li>
      
        <li><a class="flat-box" title="/friends/" href="/friends/"
          
            rel="nofollow"
          
          
          id="friends">
          
            <i class="fas fa-link fa-fw" aria-hidden="true"></i>
          
          我的友链
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/material-x/" href="https://xaoxuu.com/wiki/material-x/"
          
            rel="nofollow"
          
          
          id="https:xaoxuu.comwikimaterial-x">
          
            <i class="fas fa-book fa-fw" aria-hidden="true"></i>
          
          主题文档
        </a></li>
      
        <li><a class="flat-box" title="/about/mantch/" href="/about/mantch/"
          
            rel="nofollow"
          
          
          id="aboutmantch">
          
            <i class="fas fa-info-circle fa-fw" aria-hidden="true"></i>
          
          关于小站
        </a></li>
      
    </ul>
  </div>
</section>

            
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
            
              
  <section class='widget category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;文章分类</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/blog/categories/"
    title="blog/categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/NLP/" href="/categories/NLP/"><div class='name'>NLP</div><div class='badge'>(11)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"><div class='name'>推荐系统</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><div class='name'>机器学习</div><div class='badge'>(15)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><div class='name'>深度学习</div><div class='badge'>(8)</div></a></li>
        
      </ul>
    </div>
  </section>


            
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
            
              

            
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  
  
    <div class="social-wrapper">
      
        
          <a href="mailto:mantchs@163.com"
            class="social fas fa-envelope flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/NLP-LOVE"
            class="social fab fa-github flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>
    本站使用
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a>
    作为主题
    
      ，
      总访问量为
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      次
    
    。
  </div>
</footer>
<script>setLoadingBarProgress(80);</script>




      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>




  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
    <script type="text/javascript">
      $(function(){
        if ('.cover') {
          $('.cover').backstretch(
          ["https://img.vim-cn.com/29/91197b04c13f512f734a76d4ac422d89dbe229.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        } else {
          $.backstretch(
          ["https://img.vim-cn.com/29/91197b04c13f512f734a76d4ac422d89dbe229.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        }
      });
    </script>
  







  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script type="text/javascript">
    var gitalk = new Gitalk({
      clientID: "9e843f5ca6489d3dafe8",
      clientSecret: "4c4ecc0dc9d3f67c295c96945ada3ad65fe6d3d5",
      repo: "blog-comment",
      owner: "NLP-LOVE",
      admin: "NLP-LOVE",
      
        id: location.pathname,      // Ensure uniqueness and length less than 50
      
      distractionFreeMode: false  // Facebook-like distraction free mode
    });
    gitalk.render('gitalk-container');
  </script>





  
<script src="/js/app.js"></script>



  
<script src="/js/search.js"></script>





<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "复制成功";
  let COPY_FAILURE = "复制失败";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>复制</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
</body>
</html>
